{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import os\n",
    "from sklearn import metrics\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {\n",
    "    'seed':[],\n",
    "    'encoder':[],\n",
    "    'data':[],\n",
    "    'enhance_type':[],\n",
    "    'enhance_llm':[],\n",
    "    'accuracy':[],\n",
    "    'precision':[],\n",
    "    'recall':[],\n",
    "    'f1':[],\n",
    "    'f2':[]\n",
    "}\n",
    "for s in range(2014,2019):\n",
    "    for m in ['CodeBERT','Jina','Jina1024']:\n",
    "        for d in ['EBT','iTrust', 'RETRO','eTOUR']:\n",
    "            for e in ['none','req','code','req(example)','code(example)']:\n",
    "                for l in ['none','Claude3','Gemini','GPT3.5','GPT-4o']:\n",
    "                    try:\n",
    "                        df = pd.read_csv(f'{s}/Single-{m}-{d}-{e}-{l}-{s}-record.csv')\n",
    "                        records['seed'].append(s)\n",
    "                        records['encoder'].append(m)\n",
    "                        records['data'].append(d)\n",
    "                        records['enhance_type'].append(e)\n",
    "                        records['enhance_llm'].append(l)\n",
    "                        records['accuracy'].append(metrics.accuracy_score(df['label'], df['pred']))\n",
    "                        records['precision'].append(metrics.precision_score(df['label'], df['pred']))\n",
    "                        records['recall'].append(metrics.recall_score(df['label'], df['pred']))\n",
    "                        records['f1'].append(metrics.f1_score(df['label'], df['pred']))\n",
    "                        records['f2'].append(metrics.fbeta_score(df['label'], df['pred'], beta=2))\n",
    "                    except Exception as error:\n",
    "                        if e!='none' and l !='none':\n",
    "                            print(f'{s}/Single-{m}-{d}-{e}-{l}-{s}-record.csv')\n",
    "                            print(error)\n",
    "                        continue\n",
    "\n",
    "records = pd.DataFrame(records)\n",
    "jina_records = records[(records['encoder']=='Jina')&(records['enhance_llm']=='GPT-4o')&(records['enhance_type']=='req')]\n",
    "jina_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records.to_csv('test_records.csv',index=False)\n",
    "jina_records[['encoder','data','enhance_type','seed','accuracy','precision','recall','f1','f2']].groupby(by=['encoder','data','enhance_type']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL\n",
    "records = {\n",
    "    'seed':[],\n",
    "    'encoder':[],\n",
    "    'data':[],\n",
    "    'enhance_type':[],\n",
    "    'enhance_llm':[],\n",
    "    'accuracy':[],\n",
    "    'precision':[],\n",
    "    'recall':[],\n",
    "    'f1':[],\n",
    "    'f2':[]\n",
    "}\n",
    "for s in range(2014,2019):\n",
    "    for m in ['lstm','bi_lstm','gru','bi_gru']:\n",
    "        for d in ['EBT','iTrust', 'RETRO','eTOUR']:\n",
    "            for e in ['none','req','code','req(example)','code(example)']:\n",
    "                for l in ['none','claude3','gemini','gpt3.5','gpt4o']:\n",
    "                    try:\n",
    "                        df = pd.read_csv(f'{s}/{m}-{d}-{e}-{l}-{s}-record.csv')\n",
    "                        records['seed'].append(s)\n",
    "                        records['encoder'].append(m)\n",
    "                        records['data'].append(d)\n",
    "                        records['enhance_type'].append(e)\n",
    "                        records['enhance_llm'].append(l)\n",
    "                        records['accuracy'].append(metrics.accuracy_score(df['label'], df['pred']))\n",
    "                        records['precision'].append(metrics.precision_score(df['label'], df['pred']))\n",
    "                        records['recall'].append(metrics.recall_score(df['label'], df['pred']))\n",
    "                        records['f1'].append(metrics.f1_score(df['label'], df['pred']))\n",
    "                        records['f2'].append(metrics.fbeta_score(df['label'], df['pred'], beta=2))\n",
    "                    # 打印错误类型\n",
    "                    except Exception as error:\n",
    "                        if e!='none' and l !='none':\n",
    "                            print(f'{s}/{m}-{d}-{e}-{l}-{s}-record.csv')\n",
    "                            print(error)\n",
    "                        continue\n",
    "\n",
    "records = pd.DataFrame(records)\n",
    "dl_records=records[(records['enhance_llm']=='gpt4o')&(records['enhance_type']=='req')]\n",
    "dl_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b612c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "records[['encoder','data','enhance_type','seed','accuracy','precision','recall','f1','f2']].groupby(by=['encoder','data','enhance_type']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cbfb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "records = pd.read_excel('ml_enhance_result_record.xlsx')\n",
    "ml_records=records[(records['enhance_llm']=='gpt4o')&(records['enhance_type']=='req')]\n",
    "ml_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR\n",
    "records = {\n",
    "    'seed':[],\n",
    "    'encoder':[],\n",
    "    'data':[],\n",
    "    'enhance_type':[],\n",
    "    'enhance_llm':[],\n",
    "    'accuracy':[],\n",
    "    'precision':[],\n",
    "    'recall':[],\n",
    "    'f1':[],\n",
    "    'f2':[]\n",
    "}\n",
    "for s in range(2014,2019):\n",
    "    for m in ['LSI','LDA','VSM']:\n",
    "        for d in ['EBT','iTrust', 'RETRO','eTOUR']:\n",
    "            try:\n",
    "                df = pd.read_csv(f'{m}-{d}-record.csv')\n",
    "                records['seed'].append(s)\n",
    "                records['encoder'].append(m)\n",
    "                records['data'].append(d)\n",
    "                records['enhance_type'].append('-')\n",
    "                records['enhance_llm'].append('-')\n",
    "                records['accuracy'].append(metrics.accuracy_score(df['label'], df['pred']))\n",
    "                records['precision'].append(metrics.precision_score(df['label'], df['pred']))\n",
    "                records['recall'].append(metrics.recall_score(df['label'], df['pred']))\n",
    "                records['f1'].append(metrics.f1_score(df['label'], df['pred']))\n",
    "                records['f2'].append(metrics.fbeta_score(df['label'], df['pred'], beta=2))\n",
    "            # 打印错误类型\n",
    "            except Exception as error:\n",
    "                if e!='none' and l !='none':\n",
    "                    print(f'{m}-{d}-record.csv')\n",
    "                    print(error)\n",
    "                continue\n",
    "\n",
    "records = pd.DataFrame(records)\n",
    "ir_records=records\n",
    "ir_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "def perform_wilcoxon_test(before, after):\n",
    "\n",
    "    stat, p = wilcoxon(before, after, alternative='less')\n",
    "    return stat, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first and second place of each metric\n",
    "# Accuracy\n",
    "# EBT\n",
    "before =  ml_records[(ml_records['model_name']=='SVM') & (ml_records['data_name']=='EBT')]['Accuracy']\n",
    "after = jina_records[jina_records['data']=='EBT']['accuracy']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'Accuracy EBT: {p}')\n",
    "# eTOUR\n",
    "before =  ir_records[(ir_records['encoder']=='VSM') & (ir_records['data']=='eTOUR')]['accuracy']\n",
    "after = jina_records[jina_records['data']=='eTOUR']['accuracy']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'Accuracy eTOUR: {p}')\n",
    "# iTrust\n",
    "before =  ir_records[(ir_records['encoder']=='VSM') & (ir_records['data']=='iTrust')]['accuracy']\n",
    "after = jina_records[jina_records['data']=='iTrust']['accuracy']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'Accuracy iTrust: {p}')\n",
    "# RETRO\n",
    "before =  ml_records[(ml_records['model_name']=='LR') & (ml_records['data_name']=='RETRO')]['Accuracy']\n",
    "after = jina_records[jina_records['data']=='RETRO']['accuracy']\n",
    "stat, p = perform_wilcoxon_test(before, after)  \n",
    "print(f'Accuracy RETRO: {p}')\n",
    "# F1\n",
    "# EBT\n",
    "before =  ml_records[(ml_records['model_name']=='SVM') & (ml_records['data_name']=='EBT')]['F1']\n",
    "after = jina_records[jina_records['data']=='EBT']['f1']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'F1 EBT: {p}')\n",
    "# eTOUR\n",
    "before =  ir_records[(ir_records['encoder']=='VSM') & (ir_records['data']=='eTOUR')]['f1']\n",
    "after = jina_records[jina_records['data']=='eTOUR']['f1']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'F1 eTOUR: {p}')\n",
    "# iTrust\n",
    "before =  ml_records[(ml_records['model_name']=='SVM') & (ml_records['data_name']=='iTrust')]['F1']\n",
    "after = jina_records[jina_records['data']=='iTrust']['f1']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'F1 iTrust: {p}')\n",
    "# RETRO\n",
    "before =  ml_records[(ml_records['model_name']=='LR') & (ml_records['data_name']=='RETRO')]['F1']\n",
    "after = jina_records[jina_records['data']=='RETRO']['f1']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'F1 RETRO: {p}')\n",
    "# F2\n",
    "# EBT\n",
    "before =  ml_records[(ml_records['model_name']=='SVM') & (ml_records['data_name']=='EBT')]['F2']\n",
    "after = jina_records[jina_records['data']=='EBT']['f2']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'F2 EBT: {p}')\n",
    "# eTOUR\n",
    "before =  dl_records[(dl_records['encoder']=='bi_lstm') & (dl_records['data']=='eTOUR')]['f2']\n",
    "after = jina_records[jina_records['data']=='eTOUR']['f2']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'F2 eTOUR: {p}')\n",
    "# iTrust\n",
    "before =  ml_records[(ml_records['model_name']=='SVM') & (ml_records['data_name']=='iTrust')]['F2']\n",
    "after = jina_records[jina_records['data']=='iTrust']['f2']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'F2 iTrust: {p}')\n",
    "# RETRO\n",
    "before =  ml_records[(ml_records['model_name']=='KNN') & (ml_records['data_name']=='RETRO')]['F2']\n",
    "after = jina_records[jina_records['data']=='RETRO']['f2']\n",
    "stat, p = perform_wilcoxon_test(before, after)\n",
    "print(f'F2 RETRO: {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc8137",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_result = {\n",
    "    'encoder':[],\n",
    "    'data':[],\n",
    "    'enhance_type':[],\n",
    "    'enhance_llm':[],\n",
    "    'metrics':[],\n",
    "    'stat':[],\n",
    "    'p':[]\n",
    "}\n",
    "for m in ['CodeBERT','Jina']:\n",
    "        for d in ['EBT','iTrust', 'RETRO','eTOUR']:\n",
    "            for e in ['req','code','req(example)','code(example)']:\n",
    "                for l in ['Claude3','Gemini','GPT3.5','GPT-4o']:\n",
    "                        for metric in ['accuracy','precision','recall','f1','f2']:\n",
    "                            before = records[(records['encoder']==m) & (records['data']==d) & (records['enhance_type']=='none') & (records['enhance_llm']=='none')][metric].values\n",
    "                            after = records[(records['encoder']==m)&(records['data']==d)&(records['enhance_type']==e)&(records['enhance_llm']==l)][metric].values\n",
    "                            # print('before',before)\n",
    "                            # print('after',after)\n",
    "                            # print(m,d,e,l,metric)\n",
    "                            stat, p = perform_wilcoxon_test(before, after)\n",
    "                            p_result['encoder'].append(m)\n",
    "                            p_result['data'].append(d)\n",
    "                            p_result['enhance_type'].append(e)\n",
    "                            p_result['enhance_llm'].append(l)\n",
    "                            p_result['metrics'].append(metric)\n",
    "                            p_result['stat'].append(stat)\n",
    "                            p_result['p'].append(p)\n",
    "pd.DataFrame(p_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(p_result).to_csv('enhance_p_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82539f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def perform_wilcoxon_test(before, after):\n",
    "\n",
    "    stat, p = wilcoxon(before, after, alternative='less')\n",
    "    return stat, p\n",
    "p_result = {\n",
    "    'encoder':[],\n",
    "    'data':[],\n",
    "    'enhance_type':[],\n",
    "    'enhance_llm':[],\n",
    "    'metrics':[],\n",
    "    'stat':[],\n",
    "    'p':[]\n",
    "}\n",
    "\n",
    "for d in ['EBT','iTrust', 'RETRO','eTOUR']:\n",
    "    for e in ['none','req','code','req(example)','code(example)']:\n",
    "        for l in ['none','Claude3','Gemini','GPT3.5','GPT-4o']:\n",
    "                for metric in ['accuracy','precision','recall','f1','f2']:\n",
    "                    try:\n",
    "                        before = records[(records['encoder']=='CodeBERT') & (records['data']==d) & (records['enhance_type']==e) & (records['enhance_llm']==l)][metric].values\n",
    "                        after = records[(records['encoder']=='Jina')&(records['data']==d)&(records['enhance_type']==e)&(records['enhance_llm']==l)][metric].values\n",
    "                        print('before',before)\n",
    "                        print('after',after)\n",
    "                        print(m,d,e,l,metric)\n",
    "                        stat, p = perform_wilcoxon_test(before, after)\n",
    "                        p_result['encoder'].append('jina')\n",
    "                        p_result['data'].append(d)\n",
    "                        p_result['enhance_type'].append(e)\n",
    "                        p_result['enhance_llm'].append(l)\n",
    "                        p_result['metrics'].append(metric)\n",
    "                        p_result['stat'].append(stat)\n",
    "                        p_result['p'].append(p)\n",
    "                    except Exception as e:\n",
    "                        print('error',d,e,l,metric)\n",
    "                        print(e)\n",
    "pd.DataFrame(p_result).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(p_result).dropna().to_csv('encoder_p_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def perform_wilcoxon_test(before, after):\n",
    "\n",
    "    stat, p = wilcoxon(before, after, alternative='less')\n",
    "    return stat, p\n",
    "p_result = {\n",
    "    'encoder':[],\n",
    "    'data':[],\n",
    "    'enhance_type':[],\n",
    "    'enhance_llm':[],\n",
    "    'metrics':[],\n",
    "    'stat':[],\n",
    "    'p':[]\n",
    "}\n",
    "\n",
    "for d in ['EBT','iTrust', 'RETRO','eTOUR']:\n",
    "    for e in ['none','req','code','req(example)','code(example)']:\n",
    "        for l in ['none','Claude3','Gemini','GPT3.5','GPT-4o']:\n",
    "                for metric in ['accuracy','precision','recall','f1','f2']:\n",
    "                    try:\n",
    "                        before = records[(records['encoder']=='Jina') & (records['data']==d) & (records['enhance_type']==e) & (records['enhance_llm']==l)][metric].values\n",
    "                        after = records[(records['encoder']=='Jina1024')&(records['data']==d)&(records['enhance_type']==e)&(records['enhance_llm']==l)][metric].values\n",
    "                        print('before',before)\n",
    "                        print('after',after)\n",
    "                        print(m,d,e,l,metric)\n",
    "                        stat, p = perform_wilcoxon_test(before, after)\n",
    "                        p_result['encoder'].append('jina')\n",
    "                        p_result['data'].append(d)\n",
    "                        p_result['enhance_type'].append(e)\n",
    "                        p_result['enhance_llm'].append(l)\n",
    "                        p_result['metrics'].append(metric)\n",
    "                        p_result['stat'].append(stat)\n",
    "                        p_result['p'].append(p)\n",
    "                    except Exception as e:\n",
    "                        print('error',d,e,l,metric)\n",
    "                        print(e)\n",
    "pd.DataFrame(p_result).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(p_result).dropna().to_csv('long_p_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5147bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
