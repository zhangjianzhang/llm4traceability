{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a786a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.expand_frame_repr', False)  # \n",
    "import os\n",
    "candidates = [ '1-20_jina_result_record.xlsx',\n",
    " '1-20_ml_enhance_result_record.xlsx',\n",
    " '1-20_rnn_result.xlsx',\n",
    " '1-20_LSI_.xlsx',\n",
    " '1-20_codebert_result_record.xlsx']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "result = {\n",
    "    'model_name':[],\n",
    "    'data_name':[],\n",
    "    # 'enhance_type':[],\n",
    "    # 'enhance_llm':[],\n",
    "    'precision':[],\n",
    "    'recall':[],\n",
    "    'f1':[],\n",
    "    'f2':[],\n",
    "}\n",
    "for i in range(len(candidates)):\n",
    "    df = pd.read_excel(candidates[i])\n",
    "    \n",
    "    if 'socre' in df.columns:\n",
    "        df['score'] = df['socre']\n",
    "    \n",
    "    if 'pred' not in df.columns:\n",
    "        t = df['score'].mean()\n",
    "        df['pred'] = df['score'].apply(lambda x: 1 if x >= t else 0)\n",
    "    for d in ['EBT','iTrust','eTOUR','RETRO']:\n",
    "        df_d = df[df['data_name'] == d]\n",
    "        # accuracy precision recall f1 f2\n",
    "        precision = metrics.precision_score(df_d['label'], df_d['pred'])\n",
    "        recall = metrics.recall_score(df_d['label'], df_d['pred'])\n",
    "        f1 = metrics.f1_score(df_d['label'], df_d['pred'])\n",
    "        f2 = metrics.fbeta_score(df_d['label'], df_d['pred'], beta=2)\n",
    "        result['model_name'].append(candidates[i].split('_')[1])\n",
    "        result['data_name'].append(d)\n",
    "        result['precision'].append(precision)\n",
    "        result['recall'].append(recall)\n",
    "        result['f1'].append(f1)\n",
    "        result['f2'].append(f2)\n",
    "df_result1 = pd.DataFrame(result)\n",
    "flag = True\n",
    "df_result = df_result1[['model_name','precision','recall','f1','f2']].groupby(by=['model_name']).mean()\n",
    "df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edbe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "result = {\n",
    "    'model_name':[],\n",
    "    'data_name':[],\n",
    "    # 'enhance_type':[],\n",
    "    # 'enhance_llm':[],\n",
    "    'seed':[],\n",
    "    'precision':[],\n",
    "    'recall':[],\n",
    "    'f1':[],\n",
    "    'f2':[],\n",
    "}\n",
    "for i in range(len(candidates)):\n",
    "    df = pd.read_excel(candidates[i])\n",
    "    \n",
    "    if 'socre' in df.columns:\n",
    "        df['score'] = df['socre']\n",
    "    \n",
    "    if 'pred' not in df.columns:\n",
    "        t = df['score'].mean()\n",
    "        df['pred'] = df['score'].apply(lambda x: 1 if x >= t else 0)\n",
    "    for d in ['EBT','iTrust','eTOUR','RETRO']:\n",
    "        for seed in range(2014,2019):\n",
    "            if 'seed' in df.columns:\n",
    "                df_d = df[(df['data_name'] == d)&(df['seed'] == seed)]\n",
    "            else:\n",
    "                df_d = df[(df['data_name'] == d)]\n",
    "            # precision recall f1 f2\n",
    "            precision = metrics.precision_score(df_d['label'], df_d['pred'])\n",
    "            recall = metrics.recall_score(df_d['label'], df_d['pred'])\n",
    "            f1 = metrics.f1_score(df_d['label'], df_d['pred'])\n",
    "            f2 = metrics.fbeta_score(df_d['label'], df_d['pred'], beta=2)\n",
    "            result['model_name'].append(candidates[i].split('_')[1])\n",
    "            result['data_name'].append(d)\n",
    "            result['seed'].append(seed)\n",
    "            result['precision'].append(precision)\n",
    "            result['recall'].append(recall)\n",
    "            result['f1'].append(f1)\n",
    "            result['f2'].append(f2)\n",
    "df_result1 = pd.DataFrame(result)\n",
    "flag = True\n",
    "df_result = df_result1[['model_name','seed','precision','recall','f1','f2']].groupby(by=['model_name','seed']).mean()\n",
    "df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e495d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def perform_wilcoxon_test(before, after):\n",
    "\n",
    "    stat, p = wilcoxon(before, after, alternative='less')\n",
    "    return stat, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99082825",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = df_result.reset_index()\n",
    "p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ad588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first and second place of each metric\n",
    "\n",
    "# presision\n",
    "before = p_data[p_data['model_name']=='LSI']['precision'].values\n",
    "after = p_data[p_data['model_name']=='jina']['precision'].values\n",
    "stat,p_value = perform_wilcoxon_test(before,after)\n",
    "print('precision',p_value)\n",
    "# recall\n",
    "before = p_data[p_data['model_name']=='codebert']['recall'].values\n",
    "after = p_data[p_data['model_name']=='jina']['recall'].values\n",
    "stat,p_value = perform_wilcoxon_test(before,after)\n",
    "print('recall',p_value)\n",
    "# f1\n",
    "before = p_data[p_data['model_name']=='LSI']['f1'].values\n",
    "after = p_data[p_data['model_name']=='jina']['f1'].values\n",
    "stat,p_value = perform_wilcoxon_test(before,after)\n",
    "print('f1',p_value)\n",
    "# f2\n",
    "before = p_data[p_data['model_name']=='codebert']['f2'].values\n",
    "after = p_data[p_data['model_name']=='jina']['f2'].values\n",
    "stat,p_value = perform_wilcoxon_test(before,after)\n",
    "print('f2',p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c10b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
